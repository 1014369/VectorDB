{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is a Vector Database (VectorDB) and how is it different from\n",
        "traditional databases?**\n",
        "\n",
        "A Vector Database (VectorDB) is a database that stores data as vector embeddings, which are numerical representations of items like text, images, or audio. It is mainly used to perform similarity search, meaning it finds data that is semantically similar rather than exactly the same.\n",
        "\n",
        "The key difference from traditional databases is how data is stored and queried. Traditional databases store structured data in tables and are optimized for exact matches using SQL queries. Vector databases store high-dimensional vectors and use distance metrics (like cosine similarity) to find the closest matches, making them ideal for AI use cases such as semantic search, recommendation systems, and chatbots."
      ],
      "metadata": {
        "id": "n2yhM9ddexdX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Explain the various types of VectorDBs available and describe their\n",
        "suitability for different use cases**\n",
        "Standalone (Pure) Vector Databases are designed only for storing vectors and performing fast similarity searches. They are best suited for use cases like semantic search, recommendation systems, and RAG-based AI applications where high-performance vector retrieval is the main requirement, but complex relational queries are not needed.\n",
        "\n",
        "Hybrid Vector Databases combine vector search with metadata or attribute-based filtering. They are suitable for real-world AI applications where semantic similarity must be combined with conditions like date, category, or user information, such as AI search engines and personalized assistants.\n",
        "\n",
        "Vector-Enabled Traditional Databases are conventional databases that have added vector search capabilities. They are suitable when vector search is needed alongside existing relational or document-based workloads, allowing teams to integrate AI features without migrating to a completely new database system."
      ],
      "metadata": {
        "id": "S4drdt7GesSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: Why is Chroma DB considered important in the context of AI/ML projects?\n",
        "Describe its key features.**\n",
        "\n",
        "Chroma DB is considered important in AI/ML projects because it provides a simple and developer-friendly way to store, manage, and retrieve vector embeddings, which are essential for modern AI applications like semantic search, chatbots, and Retrieval-Augmented Generation (RAG) systems. It allows seamless integration between machine-learning models and data storage without heavy infrastructure overhead.\n",
        "\n",
        "Key features of Chroma DB include efficient vector similarity search, easy embedding storage, and tight integration with popular AI frameworks such as LangChain and LLM pipelines. It supports metadata filtering, persistent storage, and local or in-memory deployment, making it especially suitable for prototyping, experimentation, and small-to-medium-scale AI applications."
      ],
      "metadata": {
        "id": "WgFdKZnB42TC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What are the benefits of using Hugging Face Hub for generative AI tasks**\n",
        "\n",
        "Hugging Face Hub is highly beneficial for generative AI tasks because it provides easy access to a vast ecosystem of pre-trained models for text, image, audio, and multimodal generation. This allows developers to quickly experiment, fine-tune, and deploy state-of-the-art generative models without building everything from scratch.\n",
        "\n",
        "Additionally, the Hub supports model versioning, dataset hosting, and collaboration, making it ideal for research and production workflows. Its seamless integration with popular frameworks like PyTorch and TensorFlow, along with inference APIs and community support, significantly accelerates development and reduces the complexity of building generative AI applications."
      ],
      "metadata": {
        "id": "0zclkGI042dw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: Describe the process and advantages of navigating and using pre-trained\n",
        "models from the Hugging Face Hub.**\n",
        "\n",
        "The process of using pre-trained models from the Hugging Face Hub involves browsing or searching models by task, selecting a suitable pre-trained model, and loading it using the Transformers or Diffusers libraries with just a few lines of code. Developers can then directly use the model for inference or fine-tune it on their own datasets, making experimentation fast and efficient.\n",
        "\n",
        "The main advantages include saving development time, access to state-of-the-art architectures, and strong community validation through model cards and benchmarks. The Hub also provides version control, easy sharing, and integration with deployment tools, enabling reliable and scalable use of generative AI models in real-world applications."
      ],
      "metadata": {
        "id": "xr_PHHTD42l3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: Install and set up Chroma DB, and insert sample vector data for semantic\n",
        "search.**\n",
        "\n",
        "The ChromaDB can be installedby\n",
        "pip install ChromaDB"
      ],
      "metadata": {
        "id": "cb1iUBH142uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "\n",
        "# Create a Chroma client\n",
        "client = chromadb.Client()\n",
        "\n",
        "# Create a collection\n",
        "collection = client.create_collection(name=\"sample_collection\")\n",
        "\n",
        "# Insert sample data\n",
        "collection.add(\n",
        "    documents=[\n",
        "        \"Artificial Intelligence is transforming technology\",\n",
        "        \"Machine Learning is a subset of AI\",\n",
        "        \"Databases store and manage data efficiently\"\n",
        "    ],\n",
        "    metadatas=[\n",
        "        {\"topic\": \"AI\"},\n",
        "        {\"topic\": \"ML\"},\n",
        "        {\"topic\": \"DB\"}\n",
        "    ],\n",
        "    ids=[\"doc1\", \"doc2\", \"doc3\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "wfdyl2JH5dUp",
        "outputId": "87e9339a-7944-4040-bb7a-41cd0b162515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'chromadb'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1099098954.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create a Chroma client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chromadb'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Demonstrate how to download and fine-tune a Hugging Face model for a\n",
        "text generation task.**\n"
      ],
      "metadata": {
        "id": "s0GtGiTp423Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "# pip install transformers datasets torch\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "\n",
        "# Step 1: Download pre-trained model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# GPT-2 has no pad token by default\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Step 2: Prepare sample dataset\n",
        "texts = [\n",
        "    \"Artificial intelligence is transforming the world.\",\n",
        "    \"Machine learning helps systems learn from data.\"\n",
        "]\n",
        "\n",
        "dataset = Dataset.from_dict({\"text\": texts})\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=64\n",
        "    )\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
        "tokenized_dataset.set_format(\"torch\")\n",
        "\n",
        "# Step 3: Fine-tuning configuration\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=2,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=5,\n",
        "    save_steps=50,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Step 4: Train the model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637,
          "referenced_widgets": [
            "00a89eef819c40c0a4c851b987abc8c0",
            "65f462d211e34f1287d5e19737f6e8da",
            "92aab0b4f9d547ca89e5161ed142bc52",
            "f3b48642ddac43daa72b143a2ce3f815",
            "365efd99158a4427a13342e9aa0cfcf3",
            "b2a54294e22a4e50a95a49ca2d39649d",
            "4fee5018e72342acbe8d90044229fc48",
            "0b840df2827b402996f7ffe1c539effd"
          ]
        },
        "id": "b97-9CrB5qcr",
        "outputId": "94abadab-9ed3-469e-b640-c9cbf9ebb081"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00a89eef819c40c0a4c851b987abc8c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "65f462d211e34f1287d5e19737f6e8da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92aab0b4f9d547ca89e5161ed142bc52",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3b48642ddac43daa72b143a2ce3f815",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "365efd99158a4427a13342e9aa0cfcf3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2a54294e22a4e50a95a49ca2d39649d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4fee5018e72342acbe8d90044229fc48",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b840df2827b402996f7ffe1c539effd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3207645206.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4019\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4020\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4022\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4140\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4141\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"loss\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4142\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   4143\u001b[0m                     \u001b[0;34m\"The model did not return a loss from the inputs, only the following keys: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4144\u001b[0m                     \u001b[0;34mf\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Create a custom LLM using Ollama and Llama2, and run it locally for basic\n",
        "text prompts.**"
      ],
      "metadata": {
        "id": "QpFgRPCf42_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install Ollama (Linux / macOS)\n",
        "curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Step 2: Pull the Llama 2 model\n",
        "ollama pull llama2\n",
        "\n",
        "# Step 3: Create a Modelfile for a custom LLM\n",
        "cat <<EOF > Modelfile\n",
        "FROM llama2\n",
        "SYSTEM You are a helpful AI assistant that gives clear and concise answers.\n",
        "EOF\n",
        "\n",
        "# Step 4: Build the custom model\n",
        "ollama create custom-llama2 -f Modelfile\n",
        "\n",
        "# Step 5: Run the custom LLM locally\n",
        "ollama run custom-llama2\n",
        "\n",
        "# Example prompt (inside the session)\n",
        "# > Explain vector databases in simple terms"
      ],
      "metadata": {
        "id": "YTk6Mzkg53F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Implement a basic RAG (Retrieval-Augmented Generation) system using Ollama with Llama3.**"
      ],
      "metadata": {
        "id": "BWeEZsC343F5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic RAG system using Ollama (Llama3) + ChromaDB\n",
        "\n",
        "# Install dependencies\n",
        "# pip install chromadb ollama sentence-transformers\n",
        "\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import ollama\n",
        "\n",
        "# Step 1: Load embedding model\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Step 2: Create ChromaDB client and collection\n",
        "client = chromadb.Client()\n",
        "collection = client.create_collection(name=\"rag_docs\")\n",
        "\n",
        "# Step 3: Add documents to VectorDB\n",
        "documents = [\n",
        "    \"Vector databases store embeddings for semantic search.\",\n",
        "    \"Retrieval Augmented Generation improves LLM accuracy.\",\n",
        "    \"Llama models can be run locally using Ollama.\"\n",
        "]\n",
        "\n",
        "embeddings = embed_model.encode(documents).tolist()\n",
        "\n",
        "collection.add(\n",
        "    documents=documents,\n",
        "    embeddings=embeddings,\n",
        "    ids=[\"doc1\", \"doc2\", \"doc3\"]\n",
        ")\n",
        "\n",
        "# Step 4: User query\n",
        "query = \"What is RAG and why is it useful?\"\n",
        "\n",
        "query_embedding = embed_model.encode([query]).tolist()\n",
        "\n",
        "# Step 5: Retrieve relevant documents\n",
        "results = collection.query(\n",
        "    query_embeddings=query_embedding,\n",
        "    n_results=2\n",
        ")\n",
        "\n",
        "retrieved_context = \" \".join(results[\"documents\"][0])\n",
        "\n",
        "# Step 6: Generate answer using Ollama (Llama3)\n",
        "prompt = f\"\"\"\n",
        "Use the following context to answer the question.\n",
        "\n",
        "Context:\n",
        "{retrieved_context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\"\"\"\n",
        "\n",
        "response = ollama.chat(\n",
        "    model=\"llama3\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        ")\n",
        "\n",
        "print(\"Answer:\")\n",
        "print(response[\"message\"][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "3yMy2o5d59SP",
        "outputId": "fca5c56b-6060-4f50-b934-f249568af947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'chromadb'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2902782699.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# pip install chromadb ollama sentence-transformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mollama\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chromadb'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: A health-tech startup wants to build a chatbot that can answer user\n",
        "queries based on medical research articles. Propose and explain a solution using\n",
        "Hugging Face models for understanding, VectorDB for retrieval, and Ollama for\n",
        "generation.\n",
        "\n",
        "**Proposed Solution Architecture**\n",
        "\n",
        "The chatbot can be built using a Retrieval-Augmented Generation (RAG) approach, which combines document retrieval with text generation to produce accurate and context-aware answers grounded in medical research articles.\n",
        "\n",
        "**Understanding Using Hugging Face Models**\n",
        "\n",
        "Medical research articles are first processed using Hugging Face embedding models (such as sentence-transformerâ€“based models). These models convert text chunks from medical papers into vector embeddings that capture semantic meaning, enabling the system to understand user queries beyond keyword matching.\n",
        "\n",
        "\n",
        "**Retrieval Using Vector Database**\n",
        "\n",
        "All generated embeddings from the medical articles are stored in a VectorDB (such as Chroma or FAISS). When a user asks a question, the query is converted into an embedding and compared with stored vectors to retrieve the most relevant research passages. This ensures that responses are grounded in verified medical literature.\n",
        "\n",
        "\n",
        "\n",
        "**Generation Using Ollama (Local LLM)**\n",
        "\n",
        "The retrieved passages are passed as context to a locally running LLM via Ollama (for example, Llama 2). The model uses this context to generate a clear, natural-language response while staying aligned with the retrieved medical information, improving factual accuracy and reducing hallucinations.\n",
        "\n",
        "\n",
        "**Advantages of This Approach**\n",
        "\n",
        "Ensures accurate and research-backed responses\n",
        "\n",
        "Preserves data privacy by running the LLM locally\n",
        "\n",
        "Scales efficiently with large medical document collections\n",
        "\n",
        "Supports explainable answers by referencing retrieved articles"
      ],
      "metadata": {
        "id": "9PZ4iNfh43OY"
      }
    }
  ]
}